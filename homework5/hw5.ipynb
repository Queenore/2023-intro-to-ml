{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.master" : "local[1]",
        "spark.wordcount.stopwords" : "negatif"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# hw 5\n",
        "\n",
        "\n",
        "Студент Чешев А. Д.<br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703535926360,
          "endTs" : 1703535926421
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// 1. Какие плюсы и недостатки у Merge Sort Join в отличии от Hash Join? (1 балл)\r\n",
        "//\r\n",
        "// Метод Merge Sort Join эффективен в случае больших объемов данных, поскольку не требует их полной загрузки в память.\r\n",
        "// Его рационально использовать в тех случаях, где данные не могут быть хэшированы равномерно. \r\n",
        "// Однако Merge Sort Join требует предварительной сортировки данных, что может потребовать времени и ресурсов. \r\n",
        "// Этот метод может оказаться менее эффективным для небольших объемов данных.\r\n",
        "//\r\n",
        "// В свою очередь, Hash Join использует хеш-функции для соотнесения данных между таблицами. \r\n",
        "// Этот метод не требует предварительной сортировки данных и может быть эффективен в случае равномерного распределения\r\n",
        "// данных по хеш-функции. \r\n",
        "// Минус данного метода заключается в большей чувствительности к неравномерности данных, что может требовать больше\r\n",
        "// оперативной памяти при хешировании данных."
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703535929272,
          "endTs" : 1703535932991
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// 2. Соберите WordCount приложение, запустите на датасете ppkm_sentiment (1 балл)\r\n",
        "\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "import org.apache.spark.sql.{SparkSession, DataFrame}\r\n",
        "\r\n",
        "val sc = spark.sparkContext\r\n",
        "\r\n",
        "val wordCountSpark = SparkSession.builder().appName(\"WordCount\").config(sc.getConf).getOrCreate()\r\n",
        "val filePath = \"datasets/ppkmSentiment/ppkm_dataset.csv\"\r\n",
        "\r\n",
        "val reviewsDf: DataFrame = wordCountSpark.read\r\n",
        "        .option(\"header\", true)\r\n",
        "        .option(\"escape\", \"\\\"\")\r\n",
        "        .schema(\"text STRING, label INT\")\r\n",
        "        .csv(filePath)\r\n",
        "\r\n",
        "val wcDf: DataFrame = reviewsDf\r\n",
        "        .select(explode(split($\"text\", raw\"\\W+\")).as(\"word\"))\r\n",
        "        .where($\"word\" =!= \"\")\r\n",
        "        .groupBy($\"word\")\r\n",
        "        .count()\r\n",
        "        .orderBy($\"count\".desc)\r\n",
        "\r\n",
        "wcDf.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------+-----+\n",
            "|   word|count|\n",
            "+-------+-----+\n",
            "|negatif|  100|\n",
            "|positif|  100|\n",
            "| netral|  100|\n",
            "+-------+-----+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703535932995,
          "endTs" : 1703535933371
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// 3. Измените WordCount так, чтобы он удалял знаки препинания и приводил все слова к единому регистру (1 балл)\r\n",
        "\r\n",
        "var punktLowerDf: DataFrame = wordCountSpark.read\r\n",
        "        .option(\"header\", true)\r\n",
        "        .option(\"escape\", \"\\\"\")\r\n",
        "        .schema(\"text STRING, label STRING\")\r\n",
        "        .csv(filePath)\r\n",
        "        .withColumn(\"label\", lower(regexp_replace($\"label\", \"[\\\\p{Punct}]\", \"\")))\r\n",
        "\r\n",
        "punktLowerDf.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------+--------------------+\n",
            "|   text|               label|\n",
            "+-------+--------------------+\n",
            "|positif|kami siap laksana...|\n",
            "|positif|siap melaksanakan...|\n",
            "|positif|siap dukung dan s...|\n",
            "|positif|langkah 3m ini su...|\n",
            "|positif|siap amankan selu...|\n",
            "|positif|siap utk di sosia...|\n",
            "|positif|mendukung kebijak...|\n",
            "|positif|mari bersama cega...|\n",
            "|positif|mari kita suksesk...|\n",
            "|positif|siap kawal dan am...|\n",
            "|positif|semoga pak gubern...|\n",
            "|positif|semoga bapak sela...|\n",
            "|positif|     semangat selalu|\n",
            "|positif|semoga berhasilmi...|\n",
            "|positif|patuhi prokes dan...|\n",
            "|positif|terimakasih atas ...|\n",
            "|positif|sehat selalu pemi...|\n",
            "|positif|semoga selalu dib...|\n",
            "|positif|semoga pandemi se...|\n",
            "|positif|makasih bu sukses...|\n",
            "+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703535934364,
          "endTs" : 1703535935507
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// 4. Измените выход WordCount так, чтобы сортировка была по количеству повторений, а список слов был во\r\n",
        "//    втором столбце, а не в первом (1 балл)\r\n",
        "\r\n",
        "val reviewsDf: DataFrame = wordCountSpark.read\r\n",
        "        .option(\"header\", true)\r\n",
        "        .option(\"escape\", \"\\\"\")\r\n",
        "        .schema(\"text STRING, label INT\")\r\n",
        "        .csv(filePath)\r\n",
        "\r\n",
        "val wcDf: DataFrame = reviewsDf\r\n",
        "    .select(explode(split($\"text\", raw\"\\W+\")).as(\"word\"))\r\n",
        "    .where($\"word\" =!= \"\")\r\n",
        "    .groupBy($\"word\")\r\n",
        "    .count()\r\n",
        "    .orderBy($\"count\".desc)\r\n",
        "    .select($\"count\", $\"word\")\r\n",
        "\r\n",
        "wcDf.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----+-------+\n",
            "|count|   word|\n",
            "+-----+-------+\n",
            "|  100|negatif|\n",
            "|  100|positif|\n",
            "|  100| netral|\n",
            "+-----+-------+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703535937880,
          "endTs" : 1703535938925
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// 5. Измените выход WordCount, чтобы формат соответствовал TSV (1 балл)\r\n",
        "\r\n",
        "val reviewsDf: DataFrame = wordCountSpark.read\r\n",
        "        .option(\"header\", true)\r\n",
        "        .option(\"escape\", \"\\\"\")\r\n",
        "        .schema(\"text STRING, label INT\")\r\n",
        "        .csv(filePath)\r\n",
        "\r\n",
        "val wcDf: DataFrame = reviewsDf\r\n",
        "        .select(explode(split($\"text\", raw\"\\W+\")).as(\"word\"))\r\n",
        "        .where($\"word\" =!= \"\")\r\n",
        "        .groupBy($\"word\")\r\n",
        "        .count()\r\n",
        "        .orderBy($\"count\".desc)\r\n",
        "        .select(concat_ws(\"\\t\", $\"word\", $\"count\").alias(\"word_count\"))\r\n",
        "\r\n",
        "wcDf.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----------+\n",
            "| word_count|\n",
            "+-----------+\n",
            "|negatif\t100|\n",
            "|positif\t100|\n",
            "| netral\t100|\n",
            "+-----------+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703535940283,
          "endTs" : 1703535941350
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// 6. Добавьте в WordCount возможность через конфигурацию задать список стоп-слов, которые будут \r\n",
        "//    отфильтрованы во время работы приложения (2 балла)\r\n",
        "val stopWordsList = wordCountSpark.conf.get(\"spark.wordcount.stopwords\", \"\").split(\",\").map(_.trim)\r\n",
        "\r\n",
        "val reviewsDf: DataFrame = wordCountSpark.read\r\n",
        "        .option(\"header\", true)\r\n",
        "        .option(\"escape\", \"\\\"\")\r\n",
        "        .schema(\"text STRING, label INT\")\r\n",
        "        .csv(filePath)\r\n",
        "\r\n",
        "// В данном случае исключили слово \"negatif\".\r\n",
        "val wcDf: DataFrame = reviewsDf\r\n",
        "        .select(explode(split($\"text\", raw\"\\W+\")).as(\"word\"))\r\n",
        "        .where(!($\"word\".isin(stopWordsList: _*)))\r\n",
        "        .groupBy($\"word\")\r\n",
        "        .count()\r\n",
        "        .orderBy($\"count\".desc)\r\n",
        "\r\n",
        "wcDf.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------+-----+\n",
            "|   word|count|\n",
            "+-------+-----+\n",
            "|positif|  100|\n",
            "| netral|  100|\n",
            "+-------+-----+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1703535749600,
          "endTs" : 1703535750263
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// 7. Выполните broadcast join на двух датасетах, не используя метод join(). Можно использовать любые предварительные\r\n",
        "//    трансформации. В качестве исходных данных возьмите Company.csv и Company_Tweet.csv из датасета Tweets about\r\n",
        "//    the Top Companies from 2005 to 2020 (3 балла)\r\n",
        "\r\n",
        "\r\n",
        "// Решение не соответствует теребованиям.\r\n",
        "// \r\n",
        "// val filePath1 = \"datasets/TopCompaniesTweets/Company.csv\"\r\n",
        "// val filePath2 = \"datasets/TopCompaniesTweets/Company_Tweet.csv\"\r\n",
        "// val companyDF: DataFrame = wordCountSpark.read.option(\"header\", true).csv(filePath1)\r\n",
        "// val tweetDF: DataFrame = wordCountSpark.read.option(\"header\", true).csv(filePath2)\r\n",
        "//\r\n",
        "// companyDF.show()\r\n",
        "//\r\n",
        "// tweetDF.show()\r\n",
        "//\r\n",
        "// val resultDF = tweetDF.join(broadcast(companyDF), \"ticker_symbol\")\r\n",
        "//\r\n",
        "// resultDF.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------------+------------+\n",
            "|ticker_symbol|company_name|\n",
            "+-------------+------------+\n",
            "|         AAPL|       apple|\n",
            "|         GOOG|  Google Inc|\n",
            "|        GOOGL|  Google Inc|\n",
            "|         AMZN|  Amazon.com|\n",
            "|         TSLA|   Tesla Inc|\n",
            "|         MSFT|   Microsoft|\n",
            "+-------------+------------+\n",
            "\n",
            "+------------------+-------------+\n",
            "|          tweet_id|ticker_symbol|\n",
            "+------------------+-------------+\n",
            "|550803612197457920|         AAPL|\n",
            "|550803610825928706|         AAPL|\n",
            "|550803225113157632|         AAPL|\n",
            "|550802957370159104|         AAPL|\n",
            "|550802855129382912|         AAPL|\n",
            "|550802745737768960|         AAPL|\n",
            "|550797494188142592|         AAPL|\n",
            "|550797275786518528|         AAPL|\n",
            "|550797272686923776|         AAPL|\n",
            "|550796617444765696|         AAPL|\n",
            "|550795512899960832|         AAPL|\n",
            "|550795254102638593|         AAPL|\n",
            "|550795167318700033|         AAPL|\n",
            "|550795088821886976|         AAPL|\n",
            "|550793298357391360|         AAPL|\n",
            "|550793247669231616|         AAPL|\n",
            "|550793108242198528|         AAPL|\n",
            "|550791919815892992|         AAPL|\n",
            "|550791232738590720|         AAPL|\n",
            "|550790423888035840|         AAPL|\n",
            "+------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------+------------------+------------+\n",
            "|ticker_symbol|          tweet_id|company_name|\n",
            "+-------------+------------------+------------+\n",
            "|         AAPL|550803612197457920|       apple|\n",
            "|         AAPL|550803610825928706|       apple|\n",
            "|         AAPL|550803225113157632|       apple|\n",
            "|         AAPL|550802957370159104|       apple|\n",
            "|         AAPL|550802855129382912|       apple|\n",
            "|         AAPL|550802745737768960|       apple|\n",
            "|         AAPL|550797494188142592|       apple|\n",
            "|         AAPL|550797275786518528|       apple|\n",
            "|         AAPL|550797272686923776|       apple|\n",
            "|         AAPL|550796617444765696|       apple|\n",
            "|         AAPL|550795512899960832|       apple|\n",
            "|         AAPL|550795254102638593|       apple|\n",
            "|         AAPL|550795167318700033|       apple|\n",
            "|         AAPL|550795088821886976|       apple|\n",
            "|         AAPL|550793298357391360|       apple|\n",
            "|         AAPL|550793247669231616|       apple|\n",
            "|         AAPL|550793108242198528|       apple|\n",
            "|         AAPL|550791919815892992|       apple|\n",
            "|         AAPL|550791232738590720|       apple|\n",
            "|         AAPL|550790423888035840|       apple|\n",
            "+-------------+------------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    }
  ]
}